[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/getstart2/2021-04-01-getstart2.html",
    "href": "posts/getstart2/2021-04-01-getstart2.html",
    "title": "Getting started with Acoular - Part 2",
    "section": "",
    "text": "This is the second in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first post and continues by explaining some more concepts and additional methods.\nAcoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. \nTo continue, we do the same set up as in Part 1. We define TimeSamples, PowerSpectra, MicGeom, RectGrid and SteeringVector objects and set up a BeamformerBase.\n\nimport acoular\nts = acoular.TimeSamples( name=\"three_sources.h5\" )\nps = acoular.PowerSpectra( time_data=ts, block_size=128, window=\"Hanning\" )\nmg = acoular.MicGeom( from_file=\"array_64.xml\" )\nrg = acoular.RectGrid( x_min=-0.2, x_max=0.2,\n                       y_min=-0.2, y_max=0.2,\n                       z=0.3, increment=0.01 )\nst = acoular.SteeringVector( grid=rg, mics=mg )\nbb = acoular.BeamformerBase( freq_data=ps, steer=st )\n\nWe can now plot the result the same way as we already did in Part 1.\n\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nLm = acoular.L_p( bb.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n[('three_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nThe result is obviously the same as before.\nIt was computed using a variant of the beamformer that applies the diagonal removal technique. This setting is the default and it means that we ignore the information in the main diagonal of the CSM, where the auto spectra (self-cross spectra) are stored. This is not too harmful in most cases and has the advantage that the map contains less artifacts. After this explanation, we of course now want to see the result with the full CSM including the diagonal (and hopefully with those artifacts).\nWe achieve this by setting the corresponding flag of the bb object. Have a look at the documentation on BeamformerBase to understand the r_diag flag.\n\nbb.r_diag = False\n\nObviously nothing happens (right away). This is due to the lazy evaluation again. To get the result we again have to explicitly ask for it and then we can again plot it.\n\nLm = acoular.L_p( bb.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n\n\n\n\n\n\nNice! But indeed there are some artifacts that we may have mistaken for additional weak sound sources if we did not know that there are exactly three sound sources.\nChanges in other objects will also affect the beamformer. To try this out, we change the file name in the ts object. Now this object gets data from new time histories. The ps and bb objects “know” about this automatically. No need to inform these other objects about the change. We can immediately ask for the new result and it will be computed.\n\nts.name=\"two_sources.h5\"\nLm = acoular.L_p( bb.synthetic( 8000, 3 ) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nThere are only two sources. The result is different, obviously!\nNow let us try a different type of beamformer. Instead of standard beamforming, we use functional beamforming. We set up a new object bf and specify the \\(\\gamma\\) parameter which is specific to that type of beamformer. We also use the same time histories for the three sources scene as before.\n\nts.name=\"three_sources.h5\"\nbf = acoular.BeamformerFunctional( freq_data=ps, steer=st, gamma=50  )\nLm = acoular.L_p( bf.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nNote the much smaller lobes of this beamformer. In other words, the image is less blurry.\nInstead of just beamforming we can apply a deconvolution method. Deconvolution methods aim to remove all blur and artifacts from the result. The original blurry beamforming result can be understood as the convolution of the ideal image with a filter kernel (the point spread function) which is specific to the beamforming algorithm. Deconvolution is any technique that attempts to reverse this convolution.\nIn Acoular the deconvolution is achieved in a similar way to beamforming. Instead of defining a BeamformerBase type of object, we have to use a “beamformer” with deconvolution. In our example we use the CleanSC deconvolution method and BeamformerCleansc type of object. Despite the python class name we chose it is not just a beamformer.\n\nts.name=\"three_sources.h5\"\nbs = acoular.BeamformerCleansc( freq_data=ps, steer=st )\nLm = acoular.L_p( bs.synthetic( 8000, 3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 2)]\n\n\n\n\n\n\n\n\nDepending on the computer, this computation may take some seconds already. Now there is just one grid “point” per source and the image looks perfect.\nYou may have noticed that whenever a results is computed, some messages about cache files appear. This happens because all computed results are cached to a file on disk. If one ask for the same result twice, it is still computed only once to save computing time and effort. This even means at the next start of this notebook no recalculation is necessary and all results are immediately available!\nWe can try out the automatic caching mechanism by setting up another BeamformerCleansc object bs1 with all the same parameters as the bs object we did already use.\n\nbs1 = acoular.BeamformerCleansc( freq_data=ps, steer=st )\nprint( bs, bs1 )\n\n&lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945f042290&gt; &lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945efdbcb0&gt;\n\n\nNote that while we have two identical objects, they are not the same object. Thus, the bs1 does not know about the results while bs still does. If we now trigger the computation of the results for bs1 they are not actually computed, but just read from the cache. No noteable delay will happen when executing the following commands.\n\nLm = acoular.L_p( bs1.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 3)]\n\n\n\n\n\n\n\n\nThis is indeed the same result as before.\nHowever, the cache is not used when parameters are changing. This is desirable because the result may also change. We demonstrate this here by changing one parameter of the bs object.\n\nprint( bs.damp )\nbs.damp = 0.99\n\n0.6\n\n\nWith the new value, a new computation is necessary.\n\nLm = acoular.L_p( bs.synthetic( 8000, 3 ) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend())\nplt.colorbar();\n\n\n\n\n\n\n\nThis took again some time. In our simple example the result looks not much different, but there was no way to know this before.\nIn this blog post, we learned how to use alternative methods like functional beamforming and CleanSC. There are many more such methods implemented in Acoular and now you know how to check them out. The last part of this blog post series is about time domain beamforming."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Getting started with Acoular - Part 1",
    "section": "",
    "text": "This is the first in a series of three blog posts about the basic use of Acoular. It explains some fundamental concepts and walks through a simple example.\nAcoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. \nTo use Acoular, we first have to import Acoular into our notebook.\n\nimport acoular\n\nIn this example, we want to analyze time histories from 64 microphones stored in the file “three_sources.h5”. The file contains data from a measurement (actually a simulated measurement in this special case) of scene with three different sound sources.\nThis file is in HDF5 format, which is an open all purpose numerical data container file format. Besides the time histories it contains information about the sampling rate. To learn about the internal structure of the file, have a look at the file! You will need an HDF5 file viewer (e.g. https://www.hdfgroup.org/downloads/hdfview/). For now, we can skip this step and simply load the file into our notebook. The file can be found in the same directory as the notebook itself.\n\nts = acoular.TimeSamples( name=\"three_sources.h5\" )\n\nThe file is not directly opened using Python commands, but instead we use the TimeSamples class from Acoular. This class manages the data in the file in an intelligent way which even allows to use huge data files that do not fit into the memory.\nIf we inspect the ts object, we see not the data itself, but just the type and the location in memory.\n\nts\n\n&lt;acoular.sources.TimeSamples at 0x7ff0b3e48710&gt;\n\n\nWe can use this object now to answer some questions about the data:\n\nHow many channels,\nand how many samples do we have?\nWhat is the sampling frequency?\n\n\nprint( ts.numchannels, ts.numsamples, ts.sample_freq )\n\n64 51200 51200.0\n\n\nThe signal processing can either take place in the time domain or in the frequency domain. To work in the frequency domain, the time history data must be transformed into power spectra. More specifically, we need the cross spectral matrix (CSM) which contains the pairwise cross spectra of all possible combinations of two channels. The cross spectral matrix is computed using Welch’s method. For this, an FFT block size and a window type have to be chosen.\n\nps = acoular.PowerSpectra( time_data=ts, block_size=128, window=\"Hanning\" )\nps.fftfreq()\n\narray([    0.,   400.,   800.,  1200.,  1600.,  2000.,  2400.,  2800.,\n        3200.,  3600.,  4000.,  4400.,  4800.,  5200.,  5600.,  6000.,\n        6400.,  6800.,  7200.,  7600.,  8000.,  8400.,  8800.,  9200.,\n        9600., 10000., 10400., 10800., 11200., 11600., 12000., 12400.,\n       12800., 13200., 13600., 14000., 14400., 14800., 15200., 15600.,\n       16000., 16400., 16800., 17200., 17600., 18000., 18400., 18800.,\n       19200., 19600., 20000., 20400., 20800., 21200., 21600., 22000.,\n       22400., 22800., 23200., 23600., 24000., 24400., 24800., 25200.,\n       25600.])\n\n\nWe see that after the FFT we do have spectra with 400 Hz frequency spacing. For most applications this would be too coarse. We use it here to get faster processing. If we wish to have finer spacing, we need larger block sizes.\nUp to now we have defined how the processing should be done, but we did not compute the actual CSM. Nearly all expensive computations in Acoular are only done on demand using a lazy evaluation paradigm. We can trigger the computation of the cross spectral matrix by just asking for it. In this example we do want to print this already large matrix. Instead we only print its shape.\n\nps.csm.shape\n\n[('three_sources_cache.h5', 1)]\n\n\n(65, 64, 64)\n\n\nThis matrix actually has the dimensions 65 (number of frequencies) times 64 by 64 (number of microphone channels).\nTo continue with our task to generate an acoustic photograph, we need the microphone positions. In Acoular, one option is to read them from an XML file. The file looks like this:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;                                  \n&lt;MicArray name=\"array_64\"&gt;                                  \n    &lt;pos Name=\"Point 1\" x=\"0.152\" y=\"0.1141\" z=\"0\"/&gt;\n    &lt;pos Name=\"Point 2\" x=\"0.134\" y=\"0.1021\" z=\"0\"/&gt;\n...\n    &lt;pos Name=\"Point 64\" x=\"0.0218\" y=\"-0.0329\" z=\"0\"/&gt;\n&lt;/MicArray&gt;                                 \n(most of the lines are omitted here)\nA MicGeom object handles the file:\n\nmg = acoular.MicGeom( from_file=\"array_64.xml\" )\nmg.mpos\n\narray([[ 0.152 ,  0.134 ,  0.1043,  0.0596,  0.0798,  0.0659,  0.0262,\n         0.0272,  0.    ,  0.004 ,  0.0162,  0.0162,  0.004 , -0.0112,\n        -0.018 , -0.0112, -0.145 , -0.1294, -0.1242, -0.1209, -0.0828,\n        -0.0631, -0.0595, -0.034 ,  0.0056,  0.0037, -0.016 , -0.0492,\n        -0.0024,  0.0022, -0.0267, -0.0054, -0.0874, -0.0764, -0.049 ,\n        -0.0058, -0.0429, -0.0378,  0.0003, -0.0121, -0.1864, -0.1651,\n        -0.1389, -0.1016, -0.1008, -0.0809, -0.0475, -0.0369,  0.1839,\n         0.1634,  0.146 ,  0.1235,  0.1019,  0.0799,  0.0594,  0.0393,\n         0.0774,  0.0697,  0.0778,  0.0944,  0.0473,  0.0338,  0.0478,\n         0.0218],\n       [ 0.1141,  0.1021,  0.1036,  0.1104,  0.0667,  0.0497,  0.0551,\n         0.0286,  0.    , -0.0175, -0.0078,  0.0078,  0.0175,  0.0141,\n         0.    , -0.0141,  0.1228,  0.1079,  0.0786,  0.0335,  0.0629,\n         0.0531,  0.0133,  0.0202,  0.1899,  0.1685,  0.1461,  0.1155,\n         0.104 ,  0.0825,  0.0548,  0.0391, -0.1687, -0.1502, -0.1386,\n        -0.1254, -0.0947, -0.0733, -0.061 , -0.0376, -0.0368, -0.0339,\n        -0.0481, -0.0736, -0.0255, -0.0162, -0.0382, -0.014 , -0.0477,\n        -0.0411, -0.0169,  0.0223, -0.0208, -0.0205,  0.0138, -0.0034,\n        -0.1735, -0.1534, -0.1247, -0.0827, -0.0926, -0.0753, -0.0378,\n        -0.0329],\n       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ]])\n\n\nThe mg object now contains all information about the microphone positions. Now let us this to plot the microphone geometry. For plotting we use the matplotlib package.\n\n# to interact with figures\n%matplotlib notebook\nimport matplotlib.pylab as plt\n\nplt.plot( mg.mpos[0], mg.mpos[1], 'o' )\nplt.axis( 'equal' );\n\n\n\n\n\n\n\nThis gives a nice impression of the microphone arrangement which consists of 7 inertwined planar logarithmic spirals (hard to see the spirals, I admit).\nTo map the sound, we need a mapping grid. Here we construct a simple regular and rectangular grid. Note that we have to decide about the size, spacing and the distance (z coordinate) from the array. Size and spacing define the number of points in the grid. The more points in the grid, the better the resolution of the acoustic photograph, but the longer the processing will take.\n\nrg = acoular.RectGrid( x_min=-0.2, x_max=0.2,\n                       y_min=-0.2, y_max=0.2,\n                       z=0.3, increment=0.01 )\nrg.pos()\n\narray([[-0.2 , -0.2 , -0.2 , ...,  0.2 ,  0.2 ,  0.2 ],\n       [-0.2 , -0.19, -0.18, ...,  0.18,  0.19,  0.2 ],\n       [ 0.3 ,  0.3 ,  0.3 , ...,  0.3 ,  0.3 ,  0.3 ]])\n\n\nThe rg object now has all information about the grid.\nThe actual method we will use is beamforming. Basically this works by using the combined microphones as a sound receiver with a directivity that is steered to each one of the grid points in turn. One important element in beamforming and similar methods is the steering vector implemented in Acoular in the SteeringVector class. This vector “connects” grid and microphones and takes into account the environmental conditions. These conditions are defined by the speed of sound and a possible background flow. If not set explicitely, a ‘standard’ environment is created in Acoular which assumes quiescent conditions (no flow) and a speed of sound of 343 m/s (air at 20°C).\n\nst = acoular.SteeringVector( grid=rg, mics=mg )\nst.env.c\n\n343.0\n\n\nIndeed the standard speed of sound is used.\nNow, we define the method we want to use and set up a standard (basic) beamformer. For this need two ingredients: the CSM and the steering vector.\n\nbb = acoular.BeamformerBase( freq_data=ps, steer=st )\n\nRemember that Acoular uses lazy evaluation. No computation yet!\nThis means although we set up everything needed to perform beamforming, computation is postponed until the results are actually needed.\nThis will happen if we ask for the result. In this example we are interested in the sum for of all FFT frequency lines in the 3rd octave band 8000 Hz. This means we need the maps for all these frequencies and then add them together to synthetically “mimic” the result of a third octave filter for that band. The result is given as mean square sound pressure contribution at the center location of the microphone array. In Acoustics, we are usually like to have the results in the form of sound pressure levels (SPL). Acoular has a helper function L_p to compute the levels from the mean square.\n\npm = bb.synthetic( 8000, 3 )\nLm = acoular.L_p( pm )\n\n[('three_sources_cache.h5', 2)]\n\n\nThe map is now stored in the array variable pm and the array Lm holds the soundpressure levels computed from this.\n\nprint(Lm.shape)\nprint(Lm)\n\n(41, 41)\n[[-350.         -350.         -350.         ... -350.\n  -350.         -350.        ]\n [-350.         -350.           61.5185086  ... -350.\n  -350.         -350.        ]\n [  66.4266366    65.93676977   68.3501483  ... -350.\n  -350.         -350.        ]\n ...\n [-350.         -350.         -350.         ... -350.\n  -350.         -350.        ]\n [-350.           54.71250037   46.46415942 ...   48.39371023\n    58.52533737 -350.        ]\n [  58.13562703   62.27400964   61.09820493 ... -350.\n    61.88124663   61.96166613]]\n\n\nThe map has the same dimensions (41 x 41) as the grid. Any zero result in the map will be clipped to -350 dB level instead of -infinity.\nNow lets plot the map as a color-coded image with 15 dB dynamic range between both ends of the color scale.\n\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n\n\n\n\n\n\nNow we enjoy the acoustic photograph of the three sources. Although it looks a bit blurry, we can guess the location of the sources and see as well that the sources have different strength.\nThis post has demonstrated how to use Acoular to produce a sound map or acoustic photograph with beamforming. Here you can see how to change some parameters and here you can learn about Acoular and time domain processing."
  }
]