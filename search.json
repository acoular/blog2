[
  {
    "objectID": "posts/convert-input-data/convert-input-data.html",
    "href": "posts/convert-input-data/convert-input-data.html",
    "title": "How to import your data into Acoular",
    "section": "",
    "text": "Introduction\nAcoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array which is stored in an HDF5 file. This blog post explains how to convert data available in other formats into this file format. As examples for other file formats we will use both .csv (comma separated text files) and .mat (Matlab files). \nTo demonstrate how to import and convert the data, we first need to get some data. In our case we use data from Zenodo, where a 64 channel recording of a scene with three sources is available in a number different formats. We use Python’s urllib for the download. Depending on your internet speed this may take a while:\n\nimport urllib.request\nurl = 'https://zenodo.org/record/5809069/files/'\nfor filename in ('three_sources.h5','three_sources.csv',\n                 'three_sourcesv7.mat','three_sourcesv73.mat'):\n    urllib.request.urlretrieve(url+filename, filename)\n\nNow we have the same data in four different formats: Acoular’s HDF5, .csv, Matlab version &lt;= 7 and Matlab versions &gt;=7.3.\n\n\nHDF5 format\nHDF5 format is an open all purpose numerical data container file format. Data objects inside HDF5 files are stored in tree-like structure comparable to files and folders in a file system. Lets open the file and explore this structure, which is very simple in this case.\nWe use the pytables library to access the file. This the very same library used by Acoular under the hood. Alternatively Acoular can also work with h5py.\nYou could also use an HDF5 file viewer with a GUI (e.g. HDFView).\n\nimport tables\nh5file = tables.open_file('three_sources.h5', mode = 'r') # read only mode\nh5file.root\n\n/ (RootGroup) ''\n  children := ['time_data' (EArray)]\n\n\nIn its root sits just one object (one ‘child’), which is an EArray (extensible array). Lets inspect its properties:\n\nh5file.root.time_data\n\n/time_data (EArray(51200, 64)) ''\n  atom := Float32Atom(shape=(), dflt=0.0)\n  maindim := 0\n  flavor := 'numpy'\n  byteorder := 'little'\n  chunkshape := (256, 64)\n\n\nWe see that this array has the size of 51200 (samples) by 64 (channels). The values are stored as 32 bit float numbers. While less than the usual 64 bit, 32 bit accuracy is more than enough in this case and it saves file space. The data itself can be accessed just like for a numpy array. As an example, we read the first 10 samples of channel 47.\n\nh5file.root.time_data[:10,47]\n\narray([ 1.5875906 , -0.7917087 ,  3.1555338 ,  1.0036362 , -3.1655273 ,\n       -6.466202  , -0.19289835,  1.7383114 ,  6.901536  ,  2.723017  ],\n      dtype=float32)\n\n\nAlong with the data itself, the object stores also some metadata (‘attributes’).\n\nh5file.root.time_data.attrs\n\n/time_data._v_attrs (AttributeSet), 5 attributes:\n   [CLASS := 'EARRAY',\n    EXTDIM := 0,\n    TITLE := '',\n    VERSION := '1.1',\n    sample_freq := 51200.0]\n\n\nThere is one custom attribute here, which is sample_freq. It specifies the sampling frequency. In our case 51200.0 Hz.\nIf we now have data in some other format that we want to use with Acoular, there are two options: 1. We read that data and convert it into an HDF5 file that follows the specification explained. This is demonstrated in this blog post. 2. We extend Acoular to read the other file format directly. This would mean to subclass the TimeSamples class and requires some understanding of Acoular’s code and working mechanism.\n\n\nImport from CSV format\nThe first option shall now be demonstrated using .csv formatted data. Despite beeing extra inefficient this human-readable text format is widely used. The file contains the same number of floating point numbers separated by commas on each line. Some .csv files have also one or more header lines explaining the data contained in the file. In our case there are no header lines. There are multiple options how to read such file into Python. We are going to use Numpy for this. Be warned, the import of this (relatively small) 80 MByte file takes some time.\n\nimport numpy as np\ndatacsv = np.genfromtxt('three_sources.csv', delimiter=',', dtype='float32')\ndatacsv\n\narray([[-0.43654928, -4.696499  , -2.9038546 , ..., -0.39481497,\n        -3.7462494 , -3.2238567 ],\n       [ 2.2970407 , -1.9746966 , -4.089035  , ..., -3.8922982 ,\n        -4.8707275 , -3.613382  ],\n       [-2.261127  ,  1.6419717 ,  3.4066103 , ..., -0.732125  ,\n         0.22087638, -1.6310387 ],\n       ...,\n       [-1.530854  , -1.2453959 ,  1.566295  , ..., -3.9039657 ,\n        -0.00989423, -6.0220094 ],\n       [ 0.47992265,  3.8888328 , -0.15509878, ..., -1.2525555 ,\n        -2.5308452 , -3.22349   ],\n       [-1.0162828 ,  1.230733  , -2.4700263 , ..., -5.659823  ,\n        -5.2780933 , -0.36301124]], dtype=float32)\n\n\nNow the data is stored in the datacsv array. the next step is to create a new HFD5 file, store the data into that file and add the attribute for the sampling frequency.\n\nh5filecsv = tables.open_file('three_sources_from_csv.h5', mode='w', \n                             title='three_sources')\nearraycsv = h5filecsv.create_earray('/', 'time_data', obj=datacsv)\ndisplay(earraycsv)\nh5filecsv.root.time_data.set_attr('sample_freq',51200.0)\nh5filecsv.close()\n\n/time_data (EArray(51200, 64)) ''\n  atom := Float32Atom(shape=(), dflt=0.0)\n  maindim := 0\n  flavor := 'numpy'\n  byteorder := 'little'\n  chunkshape := (256, 64)\n\n\nJust as before with the original HDF5 file we now have the data in new HDF5 file that could be used as data source for Acoular. There is one possible pitfall with this approach: the data is completely read into the computer memory before beeing stored into the HDF5 file. If the data is really huge, say hundreds of channels and some minutes of recording, it might not fit into the memory. In this case, a more sophisticated approach is needed, where chunks of data are read and stored consecutively. Because we use an EArray, this is possible, but we would have to modify the code.\nAs mentioned before, there are other options to read the .csv data. One that deserves to be mentioned here is Pandas which reads a lot of different data formats.\n\n\nImport from Matlab\nFor some reason it is quite popular to store data in the format used by Matlab. However, it is important to know that despite the same extension (.mat), there are different formats. If we have any of the formats used prior to Matlab v7, then we can use Scipy to import this:\n\nfrom scipy.io import loadmat\nans = loadmat('three_sourcesv7.mat')['ans']\ndatamat7 = np.array(ans, dtype='float32')\nh5filemat7 = tables.open_file('three_sources_from_mat7.h5', mode='w', \n                             title='three_sources')\nearraymat7 = h5filemat7.create_earray('/', 'time_data', obj=datamat7)\ndisplay(earraymat7)\nh5filemat7.root.time_data.set_attr('sample_freq',51200.0)\nh5filemat7.close()\n\n/time_data (EArray(51200, 64)) ''\n  atom := Float32Atom(shape=(), dflt=0.0)\n  maindim := 0\n  flavor := 'numpy'\n  byteorder := 'little'\n  chunkshape := (256, 64)\n\n\nThe format of .mat file from version 7.3 onwards is essentially an HDF5 file itself! It just uses another file name extension. Of course the internal structure is different from what Acoular is using. However, we can open it with pytables and read the data in it.\n\nmatfile73 = tables.open_file('three_sourcesv73.mat', mode = 'r')\n# be aware of Matlab transposing the array here\ndatamat73 = np.array(matfile73.root.ans[:,:], dtype='float32').T\nh5filemat73 = tables.open_file('three_sources_from_mat73.h5', mode='w', \n                             title='three_sources')\nearraymat73 = h5filemat73.create_earray('/', 'time_data', obj=datamat73)\ndisplay(earraymat73)\nh5filemat73.root.time_data.set_attr('sample_freq',51200.0)\nh5filemat73.close()\n\n/time_data (EArray(51200, 64)) ''\n  atom := Float32Atom(shape=(), dflt=0.0)\n  maindim := 0\n  flavor := 'numpy'\n  byteorder := 'little'\n  chunkshape := (256, 64)\n\n\nThis blog post has demostrated how to import data from foreign formats into Acoular. It can also be used as a guide how to convert any other formats not explicitly mentioned here."
  },
  {
    "objectID": "posts/getstart/getstart1.html",
    "href": "posts/getstart/getstart1.html",
    "title": "Getting started with Acoular - Part 1",
    "section": "",
    "text": "Introduction\nThis is the first in a series of three blog posts about the basic use of Acoular. It explains some fundamental concepts and walks through a simple example.\nAcoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. \nTo use Acoular, we first have to import Acoular into our notebook.\n\nimport acoular\n\n\n\nMicrophone data\nIn this example, we want to analyze time histories from 64 microphones stored in the file “three_sources.h5”. The file contains data from a measurement (actually a simulated measurement in this special case) of scene with three different sound sources. Here you can see how to generate “three_sources.h5” file.\nThis file is in HDF5 format, which is an open all purpose numerical data container file format. Besides the time histories it contains information about the sampling rate. To learn about the internal structure of the file, have a look at the file! You will need an HDF5 file viewer (e.g. https://www.hdfgroup.org/downloads/hdfview/). For now, we can skip this step and simply load the file into our notebook. The file can be found in the same directory as the notebook itself.\n\nts = acoular.TimeSamples( file=\"three_sources.h5\" )\n\nThe file is not directly opened using Python commands, but instead we use the TimeSamples class from Acoular. This class manages the data in the file in an intelligent way which even allows to use huge data files that do not fit into the memory.\nIf we inspect the ts object, we see not the data itself, but just the type and the location in memory.\n\nts\n\n&lt;acoular.sources.TimeSamples at 0x764303a15670&gt;\n\n\nWe can use this object now to answer some questions about the data: * How many channels, * and how many samples do we have? * What is the sampling frequency?\n\nprint( ts.num_channels, ts.num_samples, ts.sample_freq )\n\n64 51200 51200.0\n\n\n\n\nFrequency domain\nThe signal processing can either take place in the time domain or in the frequency domain. To work in the frequency domain, the time history data must be transformed into power spectra. More specifically, we need the cross spectral matrix (CSM) which contains the pairwise cross spectra of all possible combinations of two channels. The cross spectral matrix is computed using Welch’s method. For this, an FFT block size and a window type have to be chosen.\n\nps = acoular.PowerSpectra( source=ts, block_size=128, window=\"Hanning\" )\nps.fftfreq()\n\narray([    0.,   400.,   800.,  1200.,  1600.,  2000.,  2400.,  2800.,\n        3200.,  3600.,  4000.,  4400.,  4800.,  5200.,  5600.,  6000.,\n        6400.,  6800.,  7200.,  7600.,  8000.,  8400.,  8800.,  9200.,\n        9600., 10000., 10400., 10800., 11200., 11600., 12000., 12400.,\n       12800., 13200., 13600., 14000., 14400., 14800., 15200., 15600.,\n       16000., 16400., 16800., 17200., 17600., 18000., 18400., 18800.,\n       19200., 19600., 20000., 20400., 20800., 21200., 21600., 22000.,\n       22400., 22800., 23200., 23600., 24000., 24400., 24800., 25200.,\n       25600.])\n\n\nWe see that after the FFT we do have spectra with 400 Hz frequency spacing. For most applications this would be too coarse. We use it here to get faster processing. If we wish to have finer spacing, we need larger block sizes.\nUp to now we have defined how the processing should be done, but we did not compute the actual CSM. Nearly all expensive computations in Acoular are only done on demand using a lazy evaluation paradigm. We can trigger the computation of the cross spectral matrix by just asking for it. In this example we do want to print this already large matrix. Instead we only print its shape.\n\nps.csm.shape\n\n[('three_sources_cache.h5', 1)]\n\n\n(np.int64(65), np.int64(64), np.int64(64))\n\n\nThis matrix actually has the dimensions 65 (number of frequencies) times 64 by 64 (number of microphone channels).\n\n\nMicrophone positions\nTo continue with our task to generate an acoustic photograph, we need the microphone positions. In Acoular, one option is to read them from an XML file. The file looks like this:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;                                  \n&lt;MicArray name=\"array_64\"&gt;                                  \n    &lt;pos Name=\"Point 1\" x=\"0.152\" y=\"0.1141\" z=\"0\"/&gt;\n    &lt;pos Name=\"Point 2\" x=\"0.134\" y=\"0.1021\" z=\"0\"/&gt;\n...\n    &lt;pos Name=\"Point 64\" x=\"0.0218\" y=\"-0.0329\" z=\"0\"/&gt;\n&lt;/MicArray&gt;                                 \n(most of the lines are omitted here)\nA MicGeom object handles the file:\n\nmg = acoular.MicGeom( file=\"array_64.xml\" )\nmg.pos\n\narray([[ 0.152 ,  0.134 ,  0.1043,  0.0596,  0.0798,  0.0659,  0.0262,\n         0.0272,  0.    ,  0.004 ,  0.0162,  0.0162,  0.004 , -0.0112,\n        -0.018 , -0.0112, -0.145 , -0.1294, -0.1242, -0.1209, -0.0828,\n        -0.0631, -0.0595, -0.034 ,  0.0056,  0.0037, -0.016 , -0.0492,\n        -0.0024,  0.0022, -0.0267, -0.0054, -0.0874, -0.0764, -0.049 ,\n        -0.0058, -0.0429, -0.0378,  0.0003, -0.0121, -0.1864, -0.1651,\n        -0.1389, -0.1016, -0.1008, -0.0809, -0.0475, -0.0369,  0.1839,\n         0.1634,  0.146 ,  0.1235,  0.1019,  0.0799,  0.0594,  0.0393,\n         0.0774,  0.0697,  0.0778,  0.0944,  0.0473,  0.0338,  0.0478,\n         0.0218],\n       [ 0.1141,  0.1021,  0.1036,  0.1104,  0.0667,  0.0497,  0.0551,\n         0.0286,  0.    , -0.0175, -0.0078,  0.0078,  0.0175,  0.0141,\n         0.    , -0.0141,  0.1228,  0.1079,  0.0786,  0.0335,  0.0629,\n         0.0531,  0.0133,  0.0202,  0.1899,  0.1685,  0.1461,  0.1155,\n         0.104 ,  0.0825,  0.0548,  0.0391, -0.1687, -0.1502, -0.1386,\n        -0.1254, -0.0947, -0.0733, -0.061 , -0.0376, -0.0368, -0.0339,\n        -0.0481, -0.0736, -0.0255, -0.0162, -0.0382, -0.014 , -0.0477,\n        -0.0411, -0.0169,  0.0223, -0.0208, -0.0205,  0.0138, -0.0034,\n        -0.1735, -0.1534, -0.1247, -0.0827, -0.0926, -0.0753, -0.0378,\n        -0.0329],\n       [ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  0.    ,\n         0.    ]])\n\n\nThe mg object now contains all information about the microphone positions. Now let us this to plot the microphone geometry. For plotting we use the matplotlib package.\n\n# to interact with figures\n%matplotlib widget\nimport matplotlib.pylab as plt\n\nplt.plot( mg.pos[0], mg.pos[1], 'o' )\nplt.axis( 'equal' );\n\n\n\n\n\n\n\nThis gives a nice impression of the microphone arrangement which consists of 7 intertwined planar logarithmic spirals (hard to see the spirals, I admit).\n\n\nMapping grid\nTo map the sound, we need a mapping grid. Here we construct a simple regular and rectangular grid. Note that we have to decide about the size, spacing and the distance (z coordinate) from the array. Size and spacing define the number of points in the grid. The more points in the grid, the better the resolution of the acoustic photograph, but the longer the processing will take.\n\nrg = acoular.RectGrid( x_min=-0.2, x_max=0.2,\n                       y_min=-0.2, y_max=0.2,\n                       z=0.3, increment=0.01 )\nrg.pos\n\narray([[-0.2 , -0.2 , -0.2 , ...,  0.2 ,  0.2 ,  0.2 ],\n       [-0.2 , -0.19, -0.18, ...,  0.18,  0.19,  0.2 ],\n       [ 0.3 ,  0.3 ,  0.3 , ...,  0.3 ,  0.3 ,  0.3 ]], shape=(3, 1681))\n\n\nThe rg object now has all information about the grid.\n\n\nBeamforming\nThe actual method we will use is beamforming. Basically this works by using the combined microphones as a sound receiver with a directivity that is steered to each one of the grid points in turn. One important element in beamforming and similar methods is the steering vector implemented in Acoular in the SteeringVector class. This vector “connects” grid and microphones and takes into account the environmental conditions. These conditions are defined by the speed of sound and a possible background flow. If not set explicitely, a ‘standard’ environment is created in Acoular which assumes quiescent conditions (no flow) and a speed of sound of 343 m/s (air at 20°C).\n\nst = acoular.SteeringVector( grid=rg, mics=mg )\nst.env.c\n\n343.0\n\n\nIndeed the standard speed of sound is used.\nNow, we define the method we want to use and set up a standard (basic) beamformer. For this need two ingredients: the CSM and the steering vector.\n\nbb = acoular.BeamformerBase( freq_data=ps, steer=st )\n\nRemember that Acoular uses lazy evaluation. No computation yet!\nThis means although we set up everything needed to perform beamforming, computation is postponed until the results are actually needed.\nThis will happen if we ask for the result. In this example we are interested in the sum for of all FFT frequency lines in the 3rd octave band 8000 Hz. This means we need the maps for all these frequencies and then add them together to synthetically “mimic” the result of a third octave filter for that band. The result is given as mean square sound pressure contribution at the center location of the microphone array. In Acoustics, we are usually like to have the results in the form of sound pressure levels (SPL). Acoular has a helper function L_p to compute the levels from the mean square.\n\npm = bb.synthetic( 8000, 3 )\nLm = acoular.L_p( pm )\n\n[('three_sources_cache.h5', 2)]\n\n\nThe map is now stored in the array variable pm and the array Lm holds the soundpressure levels computed from this.\n\nprint(Lm.shape)\nprint(Lm)\n\n(41, 41)\n[[-350.         -350.         -350.         ... -350.\n  -350.         -350.        ]\n [-350.         -350.           61.5185086  ... -350.\n  -350.         -350.        ]\n [  66.4266366    65.93676977   68.3501483  ... -350.\n  -350.         -350.        ]\n ...\n [-350.         -350.         -350.         ... -350.\n  -350.         -350.        ]\n [-350.           54.71250037   46.46415942 ...   48.39371023\n    58.52533737 -350.        ]\n [  58.13562703   62.27400964   61.09820493 ... -350.\n    61.88124663   61.96166613]]\n\n\nThe map has the same dimensions (41 x 41) as the grid. Any zero result in the map will be clipped to -350 dB level instead of -infinity.\nNow lets plot the map as a color-coded image with 15 dB dynamic range between both ends of the color scale.\n\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n\n\n\n\n\n\nNow we enjoy the acoustic photograph of the three sources. Although it looks a bit blurry, we can guess the location of the sources and see as well that the sources have different strength.\nThis post has demonstrated how to use Acoular to produce a sound map or acoustic photograph with beamforming. Here you can see how to change some parameters and here you can learn about Acoular and time domain processing."
  },
  {
    "objectID": "posts/getstart/getstart3.html",
    "href": "posts/getstart/getstart3.html",
    "title": "Getting started with Acoular - Part 3",
    "section": "",
    "text": "Introduction\nThis is the third and final in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first two posts and continues by explaining additional concepts to be used with time domain methods.\nAcoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources.\nTo continue, we do the same set up as in Part 1. However, as we are setting out to do some signal processing in time domain, we define only TimeSamples, MicGeom, RectGrid and SteeringVector objects but no PowerSpectra or BeamformerBase.\n\nimport acoular\nts = acoular.TimeSamples( file=\"three_sources.h5\" )\nmg = acoular.MicGeom( file=\"array_64.xml\" )\nrg = acoular.RectGrid( x_min=-0.2, x_max=0.2,\n                       y_min=-0.2, y_max=0.2,\n                       z=0.3, increment=0.01 )\nst = acoular.SteeringVector( grid=rg, mics=mg )\n\n\n\nTime domain processing\nFor processing in time domain in Acoular, one may set up “chains” of processing blocks. This is very flexible and allows for easy implementation of new algorithms or algorithmic steps. Each of the blocks acts on all channels at once. Input and output may have different numbers of channels.\nFor our task we set up the following processing chain: 1. data intake from file (TimeSamples, same as before) 2. beamforming. In the time domain this amounts to different delays that have to be applied to all channels and for all grid points, and a sum for each of the grid points. This is also known as delay-and-sum. 3. band pass filtering (the time history for each point in the map is filtered). We could skip that step in principle, but it is nice to compare the result to what we got in Parts 1 and 2 from frequency domain processing, where we did a similar approach to band pass filtering. 4. power estimation (just the square, nothing else), so that we can compute levels 5. linear average over consecutive blocks in time which makes it possible to have not one image for every sample in time, which is huge amount of (mostly useless) data, but just enough data for some images\nEach object in the processing chain is connected to its predecessor via the source parameter:\n\nbt = acoular.BeamformerTime( source=ts, steer=st )\nft = acoular.FiltOctave( source=bt, band=8000, fraction='Third octave' )\npt = acoular.TimePower( source=ft )\navgt = acoular.TimeAverage( source=pt, naverage=6400 )\n\nAnd again: lazy evaluation, nothing is computed yet.\nOnly asking for the result will initiate computing. Although this is not used in this example, it should be mentioned that the architecture allows for endless data processing from a stream of input data. To this end it is possible to replace the TimeSamples object that reads the data not from a file, but from hardware.\nDifferent to the frequency domain processing, the result is not computed in one go, but in blocks of data. These blocks have a variable length that can be defined as argument of the result function each of the processing blocks have. Note that this function is actually a Python generator, that yields a number of results we have to iterate over. This helps if one wants to process large amounts of data that do not fit into the memory at once. Iteration means we can use it in for loop and get a new data block each time we run through the loop.\nIn our example we use the loop in a Python list comprehension statement. That means we collect all blocks into a list. In our case, the blocks have length 1, i.e. one map per block.\n\nres = [r.copy() for r in avgt.result(1)]\n\nThe list res contains all maps each of which averages over 6400 samples. Now we can plot all of these maps. Because time domain processing sees the map as (number of gridpoints) channels, we have to reshape the maps so that they fit the shape of the grid.\n\n%matplotlib widget\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,7))\nfor i,r in enumerate(res):\n    pm = r[0].reshape(rg.shape)\n    Lm = acoular.L_p(pm)\n    plt.subplot(2,4,i+1)\n    plt.imshow(Lm.T, vmin=Lm.max()-15, origin='lower', extent=rg.extent )\n    plt.title('sample %i to %i' % (i*6400,i*6400+6399))\nplt.tight_layout();\n\n\n\n\n\n\n\nBecause the sources emit a stationary signal, the individual maps do look not much different. The result is also very similar to what we got with the frequency domain beamformer in Part 1.\n\n\nAlterative processing chain\nWe can assemble the processing chain also in a different way with positions 2 and 3 exchanged: 1. data intake 3. band pass filtering 2. beamforming 4. power estimation 5. linear average\nIn this case we have to be careful about the effects of filtering: Because the band pass filter comes also with a frequency dependent delay, this can disturb the work of the beamformer in case that sources are not stationary. To circumvent this, we could use a special filter FiltFiltOctave with zero delay. The disavantage of this filter is that the whole time history must be read into the memory, before the first block can be processed by the beamformer. In the present simple example it is not necessary to do this.\nWe just “rewire” the processing chain. One advantage in this case is that we only have to band pass filter 64 channels (number of microphones) instead of 1641 channels (number of grid points as in the first case.\n\nft.source = ts\nbt.source = ft\npt.source = bt\n\nAfter the new chain is set up, we can again plot the results. This time we do not use an extra list for all results, but iterate directly over the maps while plotting. Remember that this takes some time.\n\nplt.figure(figsize=(10,7))\nfor i,r in enumerate(avgt.result(1)):\n    pm = r[0].reshape(rg.shape)\n    Lm = acoular.L_p(pm)\n    plt.subplot(2,4,i+1)\n    plt.imshow(Lm.T, vmin=Lm.max()-15, origin='lower', extent=rg.extent )\n    plt.title('sample %i to %i' % (i*6400,i*6400+6399))\nplt.tight_layout();\n\n\n\n\n\n\n\nAccording to our expectations, the result looks very much the same with this alternative processing chain.\nThis concludes this series of three blog posts about first steps in Acoular. If you want to know more, look at the documentation and the examples and the reference you will find there or watch out for new blog posts to come!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/getstart/getstart2.html",
    "href": "posts/getstart/getstart2.html",
    "title": "Getting started with Acoular - Part 2",
    "section": "",
    "text": "This is the second in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first post and continues by explaining some more concepts and additional methods.\nAcoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. \nTo continue, we do the same set up as in Part 1. We define TimeSamples, PowerSpectra, MicGeom, RectGrid and SteeringVector objects and set up a BeamformerBase.\n\nimport acoular\nts = acoular.TimeSamples( file=\"three_sources.h5\" )\nps = acoular.PowerSpectra( source=ts, block_size=128, window=\"Hanning\" )\nmg = acoular.MicGeom( file=\"array_64.xml\" )\nrg = acoular.RectGrid( x_min=-0.2, x_max=0.2,\n                       y_min=-0.2, y_max=0.2,\n                       z=0.3, increment=0.01 )\nst = acoular.SteeringVector( grid=rg, mics=mg )\nbb = acoular.BeamformerBase( freq_data=ps, steer=st )\n\nWe can now plot the result the same way as we already did in Part 1.\n\n%matplotlib widget\nimport matplotlib.pyplot as plt\nLm = acoular.L_p( bb.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('three_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nThe result is obviously the same as before.\n\n\nSo far the result was computed using a variant of the beamformer that applies the diagonal removal technique. This setting is the default and it means that we ignore the information in the main diagonal of the CSM, where the auto spectra (self-cross spectra) are stored. This is not too harmful in most cases and has the advantage that the map contains less artifacts. After this explanation, we of course now want to see the result with the full CSM including the diagonal (and hopefully with those artifacts).\nWe achieve this by setting the corresponding flag of the bb object. Have a look at the documentation on BeamformerBase to understand the r_diag flag.\n\nbb.r_diag = False\n\nObviously nothing happens (right away). This is due to the lazy evaluation again. To get the result we again have to explicitly ask for it and then we can again plot it.\n\nLm = acoular.L_p( bb.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n\n\n\n\n\n\nNice! But indeed there are some artifacts that we may have mistaken for additional weak sound sources if we did not know that there are exactly three sound sources.\n\n\n\nChanges in other objects will also affect the beamformer. To try this out, we change the file name in the ts object. Now this object gets data from new time histories. The ps and bb objects “know” about this automatically. No need to inform these other objects about the change. We can immediately ask for the new result and it will be computed.\n\nts.name=\"two_sources.h5\"\nLm = acoular.L_p( bb.synthetic( 8000, 3 ) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nThere are only two sources. The result is different, obviously!\n\n\n\nNow let us try a different type of beamformer. Instead of standard beamforming, we use functional beamforming. We set up a new object bf and specify the \\(\\gamma\\) parameter which is specific to that type of beamformer. We also use the same time histories for the three sources scene as before.\n\nts.file=\"three_sources.h5\"\nbf = acoular.BeamformerFunctional( freq_data=ps, steer=st, gamma=50  )\nLm = acoular.L_p( bf.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nNote the much smaller lobes of this beamformer. In other words, the image is less blurry.\n\n\n\nInstead of just beamforming we can apply a deconvolution method. Deconvolution methods aim to remove all blur and artifacts from the result. The original blurry beamforming result can be understood as the convolution of the ideal image with a filter kernel (the point spread function) which is specific to the beamforming algorithm. Deconvolution is any technique that attempts to reverse this convolution.\nIn Acoular the deconvolution is achieved in a similar way to beamforming. Instead of defining a BeamformerBase type of object, we have to use a “beamformer” with deconvolution. In our example we use the CleanSC deconvolution method and BeamformerCleansc type of object. Despite the python class name we chose it is not just a beamformer.\n\nts.file=\"three_sources.h5\"\nbs = acoular.BeamformerCleansc( freq_data=ps, steer=st )\nLm = acoular.L_p( bs.synthetic( 8000, 3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 2)]\n\n\n\n\n\n\n\n\nDepending on the computer, this computation may take some seconds already. Now there is just one grid “point” per source and the image looks perfect.\n\n\n\nYou may have noticed that whenever a results is computed, some messages about cache files appear. This happens because all computed results are cached to a file on disk. If one ask for the same result twice, it is still computed only once to save computing time and effort. This even means at the next start of this notebook no recalculation is necessary and all results are immediately available!\nWe can try out the automatic caching mechanism by setting up another BeamformerCleansc object bs1 with all the same parameters as the bs object we did already use.\n\nbs1 = acoular.BeamformerCleansc( freq_data=ps, steer=st )\nprint( bs, bs1 )\n\n&lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945f042290&gt; &lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945efdbcb0&gt;\n\n\nNote that while we have two identical objects, they are not the same object. Thus, the bs1 does not know about the results while bs still does. If we now trigger the computation of the results for bs1 they are not actually computed, but just read from the cache. No noteable delay will happen when executing the following commands.\n\nLm = acoular.L_p( bs1.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 3)]\n\n\n\n\n\n\n\n\nThis is indeed the same result as before.\nHowever, the cache is not used when parameters are changing. This is desirable because the result may also change. We demonstrate this here by changing one parameter of the bs object.\n\nprint( bs.damp )\nbs.damp = 0.99\n\n0.6\n\n\nWith the new value, a new computation is necessary.\n\nLm = acoular.L_p( bs.synthetic( 8000, 3 ) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n\n\n\n\n\n\nThis took again some time. In our simple example the result looks not much different, but there was no way to know this before.\nIn this blog post, we learned how to use alternative methods like functional beamforming and CleanSC. There are many more such methods implemented in Acoular and now you know how to check them out. The last part of this blog post series is about time domain beamforming."
  },
  {
    "objectID": "posts/getstart/getstart2.html#introduction",
    "href": "posts/getstart/getstart2.html#introduction",
    "title": "Getting started with Acoular - Part 2",
    "section": "",
    "text": "This is the second in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first post and continues by explaining some more concepts and additional methods.\nAcoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. \nTo continue, we do the same set up as in Part 1. We define TimeSamples, PowerSpectra, MicGeom, RectGrid and SteeringVector objects and set up a BeamformerBase.\n\nimport acoular\nts = acoular.TimeSamples( file=\"three_sources.h5\" )\nps = acoular.PowerSpectra( source=ts, block_size=128, window=\"Hanning\" )\nmg = acoular.MicGeom( file=\"array_64.xml\" )\nrg = acoular.RectGrid( x_min=-0.2, x_max=0.2,\n                       y_min=-0.2, y_max=0.2,\n                       z=0.3, increment=0.01 )\nst = acoular.SteeringVector( grid=rg, mics=mg )\nbb = acoular.BeamformerBase( freq_data=ps, steer=st )\n\nWe can now plot the result the same way as we already did in Part 1.\n\n%matplotlib widget\nimport matplotlib.pyplot as plt\nLm = acoular.L_p( bb.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('three_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nThe result is obviously the same as before.\n\n\nSo far the result was computed using a variant of the beamformer that applies the diagonal removal technique. This setting is the default and it means that we ignore the information in the main diagonal of the CSM, where the auto spectra (self-cross spectra) are stored. This is not too harmful in most cases and has the advantage that the map contains less artifacts. After this explanation, we of course now want to see the result with the full CSM including the diagonal (and hopefully with those artifacts).\nWe achieve this by setting the corresponding flag of the bb object. Have a look at the documentation on BeamformerBase to understand the r_diag flag.\n\nbb.r_diag = False\n\nObviously nothing happens (right away). This is due to the lazy evaluation again. To get the result we again have to explicitly ask for it and then we can again plot it.\n\nLm = acoular.L_p( bb.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n\n\n\n\n\n\nNice! But indeed there are some artifacts that we may have mistaken for additional weak sound sources if we did not know that there are exactly three sound sources.\n\n\n\nChanges in other objects will also affect the beamformer. To try this out, we change the file name in the ts object. Now this object gets data from new time histories. The ps and bb objects “know” about this automatically. No need to inform these other objects about the change. We can immediately ask for the new result and it will be computed.\n\nts.name=\"two_sources.h5\"\nLm = acoular.L_p( bb.synthetic( 8000, 3 ) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extend() )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nThere are only two sources. The result is different, obviously!\n\n\n\nNow let us try a different type of beamformer. Instead of standard beamforming, we use functional beamforming. We set up a new object bf and specify the \\(\\gamma\\) parameter which is specific to that type of beamformer. We also use the same time histories for the three sources scene as before.\n\nts.file=\"three_sources.h5\"\nbf = acoular.BeamformerFunctional( freq_data=ps, steer=st, gamma=50  )\nLm = acoular.L_p( bf.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 1)]\n\n\n\n\n\n\n\n\nNote the much smaller lobes of this beamformer. In other words, the image is less blurry.\n\n\n\nInstead of just beamforming we can apply a deconvolution method. Deconvolution methods aim to remove all blur and artifacts from the result. The original blurry beamforming result can be understood as the convolution of the ideal image with a filter kernel (the point spread function) which is specific to the beamforming algorithm. Deconvolution is any technique that attempts to reverse this convolution.\nIn Acoular the deconvolution is achieved in a similar way to beamforming. Instead of defining a BeamformerBase type of object, we have to use a “beamformer” with deconvolution. In our example we use the CleanSC deconvolution method and BeamformerCleansc type of object. Despite the python class name we chose it is not just a beamformer.\n\nts.file=\"three_sources.h5\"\nbs = acoular.BeamformerCleansc( freq_data=ps, steer=st )\nLm = acoular.L_p( bs.synthetic( 8000, 3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 2)]\n\n\n\n\n\n\n\n\nDepending on the computer, this computation may take some seconds already. Now there is just one grid “point” per source and the image looks perfect.\n\n\n\nYou may have noticed that whenever a results is computed, some messages about cache files appear. This happens because all computed results are cached to a file on disk. If one ask for the same result twice, it is still computed only once to save computing time and effort. This even means at the next start of this notebook no recalculation is necessary and all results are immediately available!\nWe can try out the automatic caching mechanism by setting up another BeamformerCleansc object bs1 with all the same parameters as the bs object we did already use.\n\nbs1 = acoular.BeamformerCleansc( freq_data=ps, steer=st )\nprint( bs, bs1 )\n\n&lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945f042290&gt; &lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945efdbcb0&gt;\n\n\nNote that while we have two identical objects, they are not the same object. Thus, the bs1 does not know about the results while bs still does. If we now trigger the computation of the results for bs1 they are not actually computed, but just read from the cache. No noteable delay will happen when executing the following commands.\n\nLm = acoular.L_p( bs1.synthetic(8000,3) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n[('two_sources_cache.h5', 1), ('three_sources_cache.h5', 3)]\n\n\n\n\n\n\n\n\nThis is indeed the same result as before.\nHowever, the cache is not used when parameters are changing. This is desirable because the result may also change. We demonstrate this here by changing one parameter of the bs object.\n\nprint( bs.damp )\nbs.damp = 0.99\n\n0.6\n\n\nWith the new value, a new computation is necessary.\n\nLm = acoular.L_p( bs.synthetic( 8000, 3 ) )\nplt.figure()\nplt.imshow( Lm.T, origin=\"lower\", vmin=Lm.max()-15, extent=rg.extent )\nplt.colorbar();\n\n\n\n\n\n\n\nThis took again some time. In our simple example the result looks not much different, but there was no way to know this before.\nIn this blog post, we learned how to use alternative methods like functional beamforming and CleanSC. There are many more such methods implemented in Acoular and now you know how to check them out. The last part of this blog post series is about time domain beamforming."
  },
  {
    "objectID": "posts/auralization/drone-auralization-example.html",
    "href": "posts/auralization/drone-auralization-example.html",
    "title": "Drone auralization example",
    "section": "",
    "text": "Introduction\nIn addition to its capabilities to detect and analyze sources with implementations of various array methods, Acoular has several tools for synthesizing signals and simulating measurements. This can be helpful when designing an experiment and pre-testing measurement setups and algorithms without having to actually set up the hardware.\nIn this post, we’ll be using these tools to simulate the flyby of a multicopter drone as it would be experienced by an arbitrary observer. Instead of the typical microphone array with dozens of microphones, the output will be restricted to just two channels, which shall represent the “ears” of a person.\nFor applications of the simulation workflow in an array context, see also here or here.\nIn the following, we will explore how to:\n\nimplement an Acoular-compatible class for generating an artificial multicopter signal\nsimulate a source which radiates this signal, has a dipole characteristic and flies along a predefined trajectory\nexport a stereo wav file with a simple auralization of that flyby\n\nFirst, we import Acoular, NumPy, and Matplotlib’s plotting functionality\n\nimport acoular as ac\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nGenerating the drone signal\nAs Acoular does not feature a specific way to generate drone signals, we define our own signal class for doing that. It is derived from the existing SignalGenerator class.\nSignals of multicopter drones are usually very recognizable, often featuring strong tonal components caused by the rotors. In reality, the radiated noise depends on a lot of factors, which we will not consider in detail here. For now, it shall be enough to assume the signal to be dependent on the number of rotors (with respective rotational speeds) and the number of blades per rotor. From that, the blade passing frequencies are calculated as dominant components and nicely mixed with some higher harmonics and broadband noise.\nThe new class shall be called DroneSignalGenerator and its interface will allow setting a list of “revolutions per minute” values for the rotors, implicitly setting the numbers of rotors, and the already mentioned number of blades per rotor.\n\n# import traits.api to enforce data types in object parameters\nfrom traits.api import List, Int\n\nclass DroneSignalGenerator( ac.SignalGenerator ):\n    \"\"\"\n    Class for generating a synthetic multicopter drone signal. \n    This is just a basic example class for demonstration purposes \n    with only few settable and some arbitrary fixed parameters.\n    It is not intended to create perfectly realistic signals.\n    \"\"\"\n\n    # List with rotor speeds (for each rotor independently)\n    # Default: 1 rotor, 15000 rpm\n    rpm_list = List([15000,])\n\n    # Number of blades per rotor\n    # Default: 2\n    num_blades_per_rotor = Int(2)\n\n    def signal( self ):\n        \"\"\"\n        function that returns the full signal\n        \"\"\"\n        # initialize a random generator for noise generation\n        rng = np.random.default_rng(seed = 42)\n        # use 1/f² broadband noise as basis for the signal\n        wn = rng.standard_normal(self.numsamples) # normal distributed values\n        wnf = np.fft.rfft(wn) # transform to freq domain\n        wnf /= (np.linspace(0.1,1,len(wnf))*5)**2 # spectrum ~ 1/f²\n        sig = np.fft.irfft(wnf) # transform to time domain\n\n        # vector with all time instances\n        t = np.arange(self.numsamples, dtype=float) / self.sample_freq\n\n        # iterate over all rotors\n        for rpm in self.rpm_list:\n            f_base = rpm / 60 # rotor speed in Hz\n\n            # randomly set phase of rotor\n            phase = rng.uniform() * 2*np.pi\n            \n            # calculate higher harmonics up to 50 times the rotor speed\n            for n in np.arange(50)+1:\n                # if we're looking at a blade passing frequency, make it louder\n                if n % self.num_blades_per_rotor == 0:\n                    amp = 1\n                else:\n                    amp = 0.2\n\n                # exponentially decrease amplitude for higher freqs with arbitrary factor\n                amp *= np.exp(-n/10)\n                \n                # add harmonic signal component to existing signal\n                sig += amp * np.sin(2*np.pi*n * f_base * t + phase) \n\n        # return signal normalized to given RMS value\n        return sig * self.rms / np.std(sig)\n\nNow we use the newly-defined class to create an object with parameters for a specific drone signal. We choose to have a quadcopter with four rotors. As the simulation should feature the drone flying by, it makes sense for two of the rotors running at a higher speed than the other two, so as to tilt a quadcopter to let it fly in one direction. Moreover, all rotors are chosen to have slightly different rotational speeds to make it a little more realistic (15010 rpm / 14962 rpm and 13536 rpm / 13007 rpm).\nThe sample_freq and numsamples traits that we set here are inherited from the SignalGenerator base class. We use a standard sampling frequency for audio signals of 44’100 Hz here and generate a signal of a little above 10 seconds. The strength of the signal can be set via the (also inherited) rms trait, but we just use the default value of 1.0 here, so don’t have to specifically set it.\n\n# length of signal\nt_msm = 10.5 # s\n# sampling frequency\nf_sample = 44100 # Hz\n\ndrone_signal = DroneSignalGenerator(rpm_list = [15010,14962,13536,13007], \n                                    num_blades_per_rotor = 2, \n                                    sample_freq = f_sample, \n                                    numsamples = f_sample*t_msm)\n\n# If you're running the example in an interactive environment, you might want\n# to listen to the pure signal by uncommenting the two following lines:\n#from IPython.display import Audio\n#display(Audio(drone_signal.signal(),rate = f_sample))\n\nThe spectrum (PSD) of our signal looks like this:\n\nplt.figure(1,(10,3))\nplt.psd(drone_signal.signal(), \n        Fs = f_sample,\n        NFFT = 4096)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMicrophone positions\nIn addition to the signal, we need characteristics of the source itself, its movement, the environment, and the way it is observed. We start with the “observer”, i.e. a microphone array with only two microphones where the ears of a standing person might be located. The observer’s head is looking mostly towards positive \\(y\\), but also slightly to the side from which the drone will come later.\n\nm = ac.MicGeom()\nm.mpos_tot = np.array([[-0.07, 0.07], # x positions, all values in m\n                       [-0.03, 0.03], # y\n                       [ 1.7 , 1.7]]) # z\n\n\n\nFlight path\nNext, the flight path is defined. A trajectory in Acoular is defined via its “waypoints” with corresponding times. We can set as many waypoints as we want as a dictionary.\nLet’s say that our drone flies about 10 m above ground, from left to right, and a little in front of the observer. The flight speed is 16 m/s in \\(x\\) direction. The absolute speed will be a little higher, since we slightly and randomly vary the \\(z\\) and \\(y\\) positions from one waypoint to the next.\n\nflight_speed = 16 # m/s\n\n# 11 seconds trajectory, which is a little more than we have signal for\nts = np.arange(12) \n\n# initialize a random generator for path deviations\nrng = np.random.default_rng(seed = 23)\n\n# Set one waypoint each second, \nwaypoints = { t : ((t-5.5)*flight_speed,       # vary \n                     6 + rng.uniform(-0.2,0.2), # randomly vary y position up to ±0.2 m around 6 m \n                    10 + rng.uniform(-0.3,0.3)) # randomly vary z position up to ±0.3 m around 10 m height\n              for t in ts }\n\ntraj = ac.Trajectory(points = waypoints)\n\nLet’s plot the trajectory together with the observer positions, as viewed from above.\n\nplt.figure(2,(10,3))\n\n# plot observer\nplt.plot(m.mpos[0,:], m.mpos[1,:], 'rx', label = 'observer')\n\n# plot trajectory\ntimes = np.linspace(0,11,100)\nxt, yt, zt = traj.location(times)\nplt.plot(xt, yt, label = 'trajectory')\n\n# plot the predefined waypoints\nxwp, ywp, zwp = zip(*traj.points.values())\nplt.plot(xwp, ywp, '&gt;', label = 'traj. waypoints')\n\nplt.xlabel('$x$ / m')\nplt.ylabel('$y$ / m')\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\nCompared to the length of the flight path, the two observer microphones are positioned rather close to each other, so they appear almost as one position in the plot.\n\n\nDrone directivity\nNow we define an actual source. Many drones exhibit a strong directivity, so using a MovingPointSourceDipole seems a good choice here. If we don’t explicitly specify otherwise, the dipole lobes will be oriented along the z axis, which is what we want in this case. For calculating the observed sound pressure, i.e. for taking into account the sound travel path source to the observer, the object needs to know what kind of environment it will be existing in. In our case, we just define a resting fluid with a speed of sound of 343 m/s.\n\n# We'll keep the environment simple for now: just air at standard conditions with speed of sound 343 m/s\ne = ac.Environment(c=343.)\n\n# Define point source\np = ac.MovingPointSourceDipole(signal = drone_signal, # the signal of the source\n                               trajectory = traj,     # set trajectory\n                               conv_amp = True,       # take into account convective amplification\n                               mics = m,              # set the \"array\" with which to measure the sound field\n                               start = 0.5,           # observation starts 0.5 seconds after signal starts at drone\n                               env = e)               # the environment the source is moving in\n\nWith this, we defined our source. But for a little more realism, let’s add a “mirror source” to simulate ground reflections. The properties of this source are exactly the same as for the original source, except for the \\(z\\) coordinate, which is inversed here. After the definition of our mirror source, both source are combined into a joint sound field description via a SourceMixer object.\n\n# Copy the waypoints from the original source into a new trajectory, but with inverted z\nwaypoints_reflection = { time : (x, y, -z) for time, (x, y, z) in waypoints.items() }\ntraj_reflection = ac.Trajectory(points = waypoints_reflection)\n\n# Define a mirror source with the mirrored trajectory\np_reflection = ac.MovingPointSourceDipole(signal = drone_signal,        # the same signal as above\n                                          trajectory = traj_reflection, # set trajectory of mirror source\n                                          conv_amp = True,\n                                          mics = m,\n                                          start = 0.5,\n                                          env = e) \n\n# Mix the original source and the mirror source\ndrone_above_ground = ac.SourceMixer( sources = [p, p_reflection] )\n\n\n\nExport a WAV file\nNow we export a wav file of our simulation, so that we can listen to it with any audio player software. Because of the lazy evaluation paradigm, all the code above should run rather quickly, since no serious calculations were done up to now. This will change here, as the source signal is propagated sample-per-sample to generate the observed sound pressure time signals. The export may take a few minutes, so don’t be alarmed if you don’t immediately get the output.\nBefore letting the data stream be exported as wav file, however, we divert it through a TimeCache object, which already writes the data on the disk (in Acoular-compatible format). The reason for this is that we do not need to recalculate everything if we happen to need the exact same data at a later point in time.\n\n# Write data stream onto disk for later re-use. This step is not necessary if runtime isn't an issue.\ncached_signals = ac.TimeCache(source = drone_above_ground)\n\n# Prepare wav output.\n# If you don't need caching, you can directly put \"source = drone_above_ground\" here.\noutput = ac.WriteWAV(name = 'drone_flyby_with_ground_reflection.wav',\n                     source = cached_signals, \n                     channels = [0,1]) # export both channels as stereo\n\n# Start the actual export\noutput.save()\n\nThat’s it, we did a simple drone auralization using the tools available in Acoular. If you listen to the output file with properly connected stereo headphones, you should hear something that resembles a quadcopter moving from left to right.\n\n\nSpectrogram\nFinally, let’s visualize the transient signal of one channel in a spectrogram. We use Acoular’s return_result function on the output object to get the whole signal track at once instead of from a block-wise yielding generator.\n\nplt.figure(3,(10,5))\nplt.specgram(ac.tools.return_result(output)[:,0], \n             Fs = f_sample,\n             noverlap = 4096-256,\n             NFFT = 4096,\n             vmin=-100,\n             vmax=-50)\nplt.ylim(0,5000)\nplt.colorbar()\nplt.xlabel('$t$ / s')\nplt.ylabel('$f$ / Hz')\nplt.show()\n\n\n\n\n\n\n\n\nAlthough this simulation is comparativly simple and far from realistic, the spectrogram already exhibits many of the features observed in actual measurement data of drone flybys: the frequency shift due to the Doppler effect, higher levels when the drone is close to the observer, and interference patterns due to ground reflections.\n\n#hide\nplt.figure(4,(7,5))\nplt.specgram(ac.tools.return_result(output)[:,0], \n             Fs = f_sample,\n             noverlap = 4096-256,\n             NFFT = 4096,\n             vmin=-100,\n             vmax=-50)\nplt.ylim(0,5000)\nplt.colorbar()\nplt.xlabel('$t$ / s')\nplt.ylabel('$f$ / Hz')\nplt.savefig(\"thumb_drone_spectrogram.png\", transparent=True, dpi=50)"
  }
]